= Few More Concepts on Model Free Prediction =
:doctype: book
:stem: latexmath
:eqnums:
:toc:

== Certainty Equivalence Estimate ==
Model based policy evaluation with model estimated from samples.

In the model free prediction methods (MC and TD Prediction methods), we generated samples using a policy stem:[\pi], and using these samples we estimated stem:[V^{\pi}(s)]. Instead of estimating stem:[V^{\pi}(s)], given an experience quadruple stem:[(s,a,s')], we can estimate stem:[\hat{\mathcal{P}}(s' \, | \, s,a)] and stem:[\hat{\mathcal{R}}(s,a,s')].

Suppose we have stem:[K] trajectories and stem:[L_k] time steps in each trajectory. Compute MLE of the transition probability:

[stem]
++++
\hat{\mathcal{P}}(s' \, | \, s,a) = \frac{1}{N(s,a)} \sum_{k=1}^K \sum_{t=1}^{L_k -1} \mathbb{I}(s_{k,t}=s, a_{k,t}=a, s_{k,t+1}=s')
++++

We should calculate this for all stem:[s' \in \mathcal{S}] given each stem:[(s,a)].

Here stem:[N(s,a)] represents the number of times we have taken the action stem:[a] at state stem:[s]. The indicator function is 1 for those instances where we have taken the action stem:[a] at state stem:[s] and transitioned to stem:[s']. Therefore, this gives the empirical probability of transitioning to stem:[s'] by taking action stem:[a] at stem:[s]. Similarly, we can compute the MLE of the reward function

[stem]
++++
\hat{\mathcal{R}}(s,a,s') = \frac{1}{N(s,a, s')} \sum_{k=1}^K \sum_{t=1}^{L_k -1} \mathbb{I}(s_{k,t}=s, a_{k,t}=a, s_{k,t+1}=s') r_{k,t}
++++

Once MLE estimates of stem:[\hat{\mathcal{P}}] and stem:[\hat{\mathcal{R}}] are known, we can use the value iteration algorithm (the DP method) or the matrix inversion method to compute a MLE based estimate for stem:[V^{\pi}(s)].

== Off-Policy Learning ==
In all our previous methods, we estimated stem:[V^{\pi}] using samples generated from policy stem:[\pi]. Can we estimate the value stem:[V^{\pi}] or stem:[Q^{\pi}] of a target policy stem:[\pi] using samples generated from another stem:[\mu] (called as behavior policy)? That is, trajectories are sampled from stem:[\mu], but the expected values have to be estimated with respect to stem:[\pi].

In the grid world example, being at state stem:[s], the behavior policy may suggest that we take 'left' action. The corresponding reward and the trajectories (samples) are given to us. Now suppose we have a target policy stem:[\pi] that suggest we take 'right' action at state stem:[s]. We need to evaluate the target policy (that is, the value of taking this action at stem:[s]) using the samples from the behavior policy.

If we were able to do this, the possible benefits are:

* For certain tasks, sample generation might be costly or impossible. In those cases, we can re-use previous experience generated from other policies.
* Learn about multiple policies while following one behavior policy.
* Learn about the optimal policy stem:[V_*] by following any exploratory (random) policy.

If we learn about a policy (stem:[V^{\pi}] or stem:[Q^{\pi}]) using samples generated from the same policy stem:[\pi], then it is called *on-policy* learning. If we learn about a policy (stem:[V^{\pi}] or stem:[Q^{\pi}]) using samples generated from some other policy stem:[\mu], then it is called *off-policy* learning.

This decouples the data collection process from the policy training process, allowing the agent to learn from a wide range of experiences, including past data or data from other agents. In summary, off-policy learning is like reading a cookbook and learning the optimal recipes without having to personally try every single potential (and possibly bad) action in the real kitchen first.

== Importance Sampling ==
Let stem:[p(\mathbf{x})] be the target distribution and stem:[q(\mathbf{x})] be the behavior distribution for some random variable stem:[\mathbf{x}]. Suppose we need to find the expectation of stem:[f(\mathbf{x})] when stem:[\mathbf{x}] is sampled from stem:[p].

[stem]
++++
\begin{align*}
\mathbb{E}_{\mathbf{x} \sim p} [f(\mathbf{x})] & = \sum_{\mathbf{x} \in S} p(\mathbf{x}) \, f(\mathbf{x}) \\

& = \sum_{\mathbf{x} \in S} q(\mathbf{x}) \left[ \frac{p(\mathbf{x})}{q(\mathbf{x})} f(\mathbf{x}) \right] \\

& = \mathbb{E}_{\mathbf{x} \sim q} \left[ \frac{p(\mathbf{x})}{q(\mathbf{x})} f(\mathbf{x})\right] \\

& \simeq \frac{1}{n} \sum_{i=1}^n \left[ \frac{p(\mathbf{x}_i)}{q(\mathbf{x}_i)} f(\mathbf{x}_i) \right], \,\,\, \mathbf{x}_i \sim q

\end{align*}
++++

Here stem:[S] is the support of the random variable stem:[\mathbf{x}]. The support of stem:[q] should cover the support of stem:[p]. This requires the ratio stem:[\frac{p(\mathbf{x})}{q(\mathbf{x})}] is well-defined everywhere that stem:[p(\mathbf{x}) > 0]. That means, stem:[q] should not assign zero probability to any outcome that is assigned non-zero probability by stem:[p]. Suppose there exists some region stem:[A] where stem:[p(\mathbf{x}) > 0] and stem:[q(\mathbf{x}) = 0]. The importance sampling formula would ignore stem:[A], because we never sample from it (we don't sample any data points where stem:[q(\mathbf{x}) = 0]). The ratio is undefined on stem:[A], so the equality breakdowns, and we cannot rewrite the expectation correctly.

We have samples of stem:[\mathbf{x}] drawn from stem:[q], but we wish to estimate the expectation of stem:[f(\mathbf{x})] under stem:[p]. The ratio stem:[ \frac{p(\mathbf{x}_i)}{q(\mathbf{x}_i)}] is the importance sampling weight for stem:[\mathbf{x}_i] as they represent the importance given to the stem:[i]th sample.

Let's denote this as the importance sampling estimator stem:[\hat{\mu}_q = \frac{1}{n} \sum_{i=1}^n \left[ \frac{p(\mathbf{x}_i)}{q(\mathbf{x}_i)} f(\mathbf{x}_i) \right\]]. The variance of this estimator is given by

[stem]
++++
\begin{align*}
\text{Var}_q(\hat{\mu}_q) & = \text{Var}_q \left[ \frac{p(\mathbf{x})}{q(\mathbf{x})} f(\mathbf{x}) \right]  \text{  as they are i.i.d} \\

& = \mathbb{E}_q \left[ \left(\frac{p(\mathbf{x})}{q(\mathbf{x})} f(\mathbf{x}) \right)^2\right]  - \left( \mathbb{E}_q \left[ \frac{p(\mathbf{x})}{q(\mathbf{x})} f(\mathbf{x}) \right] \right)^2 \\

& = \mathbb{E}_p \left[ \frac{p(\mathbf{x})}{q(\mathbf{x})} f(\mathbf{x})^2  \right] - \left[ \mathbb{E}_p (f(\mathbf{x}))\right]^2

\end{align*}
++++

If the likelihood ratio stem:[\frac{p(\mathbf{x})}{q(\mathbf{x})}] is large, the variance of the estimator explodes. The ratio will be high when stem:[p(\mathbf{x}) \approx 0.9] and stem:[q(\mathbf{x}) \approx 0.01] for the sample stem:[\mathbf{x}].

Policies are just probability distributions. We are at stem:[s], and we want to estimate stem:[V^{\pi}(s)]. In the usual TD method, we generate samples stem:[(s_t, a_t, s_{t+1}, r_{t+1})] using the policy stem:[\pi] to do it. That is, the action will be taken as per the policy stem:[\pi]. But we now have many such samples stem:[(s_t, a_t, s_{t+1}, r_{t+1})] generated using a behavior policy stem:[\mu]. We want to use these samples to estimate stem:[V^{\pi}(s)].

[stem]
++++
\begin{align*}
V^{\pi}(s) & = \mathbb{E}_{\pi} \left[ r_{t+1} + \gamma V^{\pi}(s_{t+1}) \, | \, S_t = s \right] \\

& = \sum_{a \in \mathcal{A}} \pi(a \, | \, s) \cdot \sum_{s'} \mathcal{P}^{a}_{ss'} \left( \mathcal{R}_{ss'}^a + \gamma V^{\pi}(s') \right) \\

& = \sum_{a \in \mathcal{A}} \frac{\pi(a \, | \, s)}{\mu(a \, | \, s)} \cdot \mu(a \, | \, s) \cdot \sum_{s'} \mathcal{P}^{a}_{ss'} \left( \mathcal{R}_{ss'}^a + \gamma V^{\pi}(s') \right) \\

& = \mathbb{E}_{\mu} \left[ \frac{\pi(a \, | \, s)}{\mu(a \, | \, s)} \left( r_{t+1} + \gamma V^{\pi}(s_{t+1}) \right) \, | \, S_t = s \right], \,\, a \sim \mu(\cdot \, | \, s) \\

\end{align*}
++++

Once the action stem:[a] is chosen, both policies induce the same distribution over stem:[(S_{t+1}, R_{t+1})]. Therefore, no importance sampling correction is needed for the next state or reward. The only adjustment is for the fact that the behavior policy stem:[\mu] may not choose the same actions as the target policy stem:[\pi].

Now, the TD targets stem:[r_{t+1} + \gamma V^{\pi}(s_{t+1})] are generated following the policy stem:[\mu]. Using this we can evaluate the target policy stem:[\pi]. In the update step, we weigh each TD target by the importance sampling factor:

[stem]
++++
V(s_t) \leftarrow V(s_t) + \alpha_t \left[ \frac{\pi(a_t \, | \, s_t)}{\mu(a_t \, | \, s_t)} \left( r_{t+1} + \gamma V(s_{t+1}) \right)- V(s_t) \right], \,\, a_t \sim \mu(\cdot \, | \, s_t)
++++

For a given action stem:[a_t], we will be able to calculate stem:[\pi(a_t \, | \, s_t)] as we know the target policy stem:[\pi]. This is the update rule for the off-policy TD prediction. The case stem:[\mu=\pi] is the on-policy learning. The case stem:[\mu \ne \pi] is the off-policy learning.

The condition of stem:[\frac{p(\mathbf{x})}{q(\mathbf{x})}] well-defined everywhere that stem:[p(\mathbf{x}) > 0] translates to: stem:[\mu] should not assign zero probability to action that is assigned non-zero probability by stem:[\pi]. So, stem:[\pi] may be deterministic and greedy, but stem:[\mu] should be stochastic and exploratory. Only when stem:[\mu] is stochastic, we will have good variations in the generated samples.

The variance of this estimate will be high if stem:[\pi] assigns a high probability and stem:[\mu] assigns a low probability to actions.